
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Welcome to gproc’s documentation! &#8212; gproc 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="static/documentation_options.js"></script>
    <script src="static/jquery.js"></script>
    <script src="static/underscore.js"></script>
    <script src="static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-gproc.laplace">
<span id="welcome-to-gproc-s-documentation"></span><h1>Welcome to gproc’s documentation!<a class="headerlink" href="#module-gproc.laplace" title="Permalink to this headline">¶</a></h1>
<p>Provides functions for performing Laplace approximations to GP classification posteriors.</p>
<dl class="py function">
<dt class="sig sig-object py" id="gproc.laplace.approximate_marginal_likelihood">
<span class="sig-prename descclassname"><span class="pre">gproc.laplace.</span></span><span class="sig-name descname"><span class="pre">approximate_marginal_likelihood</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_fcn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.laplace.approximate_marginal_likelihood" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the approximate marginal likelihood.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>y</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>kernel_fcn</strong> (<em>function: N x D numpy array</em><em>, </em><em>N x D numpy array</em><em>, </em><em>dictionary -&gt; N x N numpy array</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>approx_ml</strong> – The approximate marginal likelihood for the provided kernel function</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.laplace.laplace_approximation_probit">
<span class="sig-prename descclassname"><span class="pre">gproc.laplace.</span></span><span class="sig-name descname"><span class="pre">laplace_approximation_probit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">observed_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_gram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.laplace.laplace_approximation_probit" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the laplace approximation to the latent function implied by the model:</p>
<div class="math notranslate nohighlight">
\[p(y_i | f_i) = \Phi(y_i * f_i)\]</div>
<div class="math notranslate nohighlight">
\[p(f | K) = \mathcal{N}(0, K)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Phi\)</span> is the standard normal CDF and K is a gram matrix.</p>
<p>We target the posterior:</p>
<div class="math notranslate nohighlight">
\[p(f | y) \propto p(y | f)p(f | gram)\]</div>
<p>with the Laplace approximation <span class="math notranslate nohighlight">\(q(f) = \mathcal{N}(\mu, \Sigma)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>inverse_gram</strong> (<em>num_observations x num_observations numpy array</em>) – the precomputed inverse gram matrix corresponding to the observations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><em>returns proposed_f: num_observations x 1 numpy array, the mean of the Laplace approximation</em></p></li>
<li><p>returns df_ll: num_observations x 1 numpy array, the gradient of the log-likelihood wrt each <span class="math notranslate nohighlight">\(f_i\)</span></p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.laplace.laplace_predict">
<span class="sig-prename descclassname"><span class="pre">gproc.laplace.</span></span><span class="sig-name descname"><span class="pre">laplace_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">new_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_gram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">laplace_mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">laplace_cov</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">df_ll</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_fcn=&lt;function</span> <span class="pre">squared_exponential&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_params={}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_samples=1000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.laplace.laplace_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the laplace approximation to the posterior over the latent funcitons.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>new_x</strong> (<em>M x D numpy array</em>) – </p></li>
<li><p><strong>x</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>gram</strong> (<em>N x N numpy array</em>) – </p></li>
<li><p><strong>inverse_gram</strong> (<em>N x N numpy array</em>) – </p></li>
<li><p><strong>laplace_mean</strong> (<em>N numpy vector</em>) – </p></li>
<li><p><strong>laplace_cov</strong> (<em>N x N numpy array</em>) – </p></li>
<li><p><strong>df_ll</strong> (<em>num_observations x 1 numpy array</em>) – the gradient of the log-likelihood wrt each <span class="math notranslate nohighlight">\(f_i\)</span></p></li>
<li><p><strong>kernel_fcn</strong> (<em>function: N x D numpy array</em><em>, </em><em>N x D numpy array</em><em>, </em><em>dictionary -&gt; N x N numpy array</em>) – </p></li>
<li><p><strong>pred_samples</strong> (<em>float</em>) – number of samples to generate from the predictive distribution corresponding to the laplace approximation
to the latent function posterior</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>predictive_mean</strong> (<em>M numpy vector</em>) – mean of predictive distribution</p></li>
<li><p><strong>predictive_cov</strong> (<em>M x M numpy array</em>) – covariance of predictive distribution</p></li>
<li><p><strong>predictive_y</strong> (<em>M numpy vector</em>) – averaged prediction</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-gproc.generative"></span><p>Functions for generating example classification problems.</p>
<dl class="py function">
<dt class="sig sig-object py" id="gproc.generative.sample_at_x">
<span class="sig-prename descclassname"><span class="pre">gproc.generative.</span></span><span class="sig-name descname"><span class="pre">sample_at_x</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=&lt;bound</span> <span class="pre">method</span> <span class="pre">rv_continuous.cdf</span> <span class="pre">of</span> <span class="pre">&lt;scipy.stats._continuous_distns.norm_gen</span> <span class="pre">object&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.generative.sample_at_x" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples a class label at each provided input point according to the GP classification model.
Allows arbitrary kernel function and likelihood.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>N x D numpy array</em>) – Array of the input locations at which to draw samples.</p></li>
<li><p><strong>kernel_fcn</strong> (fcn: <span class="math notranslate nohighlight">\(\mathbb{R}^{N × D} × \mathbb{R}^{M × D} → \mathbb{R}^{N × M}\)</span>) – Kernel function which produces gram matrix given two sets of inputs</p></li>
<li><p><strong>kernel_params</strong> (<em>dict</em>) – kwargs to pass on to the kernel function</p></li>
<li><p><strong>likelihood</strong> (fcn: <span class="math notranslate nohighlight">\(\mathbb{R}^{N} → (0, 1)^{N}\)</span>) – The likelihood of a positive samples given the latent function <span class="math notranslate nohighlight">\(p(y=1 | f)\)</span></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>y</strong> (<em>N x 1 numpy integer array</em>) – The sampled class labels, either +1 or -1</p></li>
<li><p><strong>prob_y</strong> (<em>N x 1 numpy array</em>) – The probability of a positive sample at each point</p></li>
<li><p><strong>f</strong> (<em>N x 1 numpy array</em>) – The latent function value at each input point.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-gproc.kernels"></span><p>A number of python kernels, as defined in <a class="reference external" href="https://www.cs.toronto.edu/~duvenaud/cookbook/">https://www.cs.toronto.edu/~duvenaud/cookbook/</a></p>
<dl class="py exception">
<dt class="sig sig-object py" id="gproc.kernels.InvertError">
<em class="property"><span class="pre">exception</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">InvertError</span></span><a class="headerlink" href="#gproc.kernels.InvertError" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.kernels.chol_inverse">
<span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">chol_inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symmetric_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.kernels.chol_inverse" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Cholesky decomposition x=LL^T, and uses this to compute the inverse of X.
Only valid for symmetric x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>symmetric_x</strong> (<em>num_observations x num_observations numpy array</em>) – no symmetry checks are performed</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>x_inv</strong> (<em>num_observations x num_observations numpy array</em>) – <span class="math notranslate nohighlight">\(x^{-1}\)</span>, the inverse of symmetric_x</p></li>
<li><p><strong>chol</strong> (<em>num_observations x num_observations numpy array</em>) – matrix with lower triangular cholesky factor inside</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.kernels.linear">
<span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constant_variance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.kernels.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>A linear kernel is a non-stationary kernel, which when used with a GP, is equivalent to
Bayesian linear regression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_1</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>x_2</strong> (<em>M x D numpy array</em>) – </p></li>
<li><p><strong>constant_variance</strong> (<em>float</em>) – Controls height of function at 0</p></li>
<li><p><strong>variance</strong> (<em>float</em>) – The average distance from the mean</p></li>
<li><p><strong>offset</strong> (<em>float</em>) – x coordinate of location all functions go through</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – The corresponding kernel matrix <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>N x M matrix,</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.kernels.locally_periodic">
<span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">locally_periodic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengthscale_sqe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengthscale_p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.kernels.locally_periodic" title="Permalink to this definition">¶</a></dt>
<dd><p>A squared exponential kernel multiplied by a periodic kernel. Allows one to model periodic functions
which can vary slowly over time.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_1</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>x_2</strong> (<em>M x D numpy array</em>) – </p></li>
<li><p><strong>lengthscale</strong> (<em>float</em>) – Lower lengthscale gives “wigglier” functions</p></li>
<li><p><strong>variance</strong> (<em>float</em>) – The average distance from the mean</p></li>
<li><p><strong>period</strong> (<em>float</em>) – Distance between repititions</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – The corresponding kernel matrix <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>N x M matrix,</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.kernels.periodic">
<span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">periodic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengthscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.kernels.periodic" title="Permalink to this definition">¶</a></dt>
<dd><p>The periodic kernel allows one to model functions which repeat themselves exactly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_1</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>x_2</strong> (<em>M x D numpy array</em>) – </p></li>
<li><p><strong>lengthscale</strong> (<em>float</em>) – Lower lengthscale gives “wigglier” functions</p></li>
<li><p><strong>variance</strong> (<em>float</em>) – The average distance from the mean</p></li>
<li><p><strong>period</strong> (<em>float</em>) – Distance between repititions</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – The corresponding kernel matrix <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>N x M matrix,</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.kernels.rational_quadratic">
<span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">rational_quadratic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengthscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weighting</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.kernels.rational_quadratic" title="Permalink to this definition">¶</a></dt>
<dd><p>Rational Quadratic Kernel, equivalent to adding together many Squared Exponential kernels with different
lengthscales. Weight parameter determine relative weighting of large and small scale variations. When
the weighting goes to infinity, RQ = SE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_1</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>x_2</strong> (<em>M x D numpy array</em>) – </p></li>
<li><p><strong>lengthscale</strong> (<em>float</em>) – Lower lengthscale gives “wigglier” functions</p></li>
<li><p><strong>variance</strong> (<em>float</em>) – The average distance from the mean</p></li>
<li><p><strong>weighting</strong> (<em>float</em>) – Relative weighting of large and small scale variations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – The corresponding kernel matrix <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>N x M matrix,</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.kernels.squared_exponential">
<span class="sig-prename descclassname"><span class="pre">gproc.kernels.</span></span><span class="sig-name descname"><span class="pre">squared_exponential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengthscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">variance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.kernels.squared_exponential" title="Permalink to this definition">¶</a></dt>
<dd><p>Also known as RBF.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_1</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>x_2</strong> (<em>M x D numpy array</em>) – </p></li>
<li><p><strong>lengthscale</strong> (<em>float</em>) – Lower lengthscale gives “wigglier” functions</p></li>
<li><p><strong>variance</strong> (<em>float</em>) – The average distance from the mean</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – The corresponding kernel matrix <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>N x M matrix,</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-gproc.classifiers"></span><p>Provides “fit_classifier” style functions, which take training data and
return functions which make predictions for new x values.</p>
<dl class="py function">
<dt class="sig sig-object py" id="gproc.classifiers.bayesian_logreg">
<span class="sig-prename descclassname"><span class="pre">gproc.classifiers.</span></span><span class="sig-name descname"><span class="pre">bayesian_logreg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_chains</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_burn_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.classifiers.bayesian_logreg" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates parameters for a Bayesian logistic regression model using Stan.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_train</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>y_train</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>num_chains</strong> (<em>int</em>) – The number of independent chains to sample.</p></li>
<li><p><strong>num_samples</strong> (<em>int</em>) – The number of samples in each chain</p></li>
<li><p><strong>num_burn_in</strong> (<em>int</em>) – The number of samples to throw away at the start of the chain</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>predict</strong> (<em>function</em>) – Estimates probability of positive given test inputs</p></li>
<li><p><strong>fit_info</strong> (<em>dict</em>) – Dictionary of arbitrary info on the specific fit</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.classifiers.full_bayesian_pseudo_marginal">
<span class="sig-prename descclassname"><span class="pre">gproc.classifiers.</span></span><span class="sig-name descname"><span class="pre">full_bayesian_pseudo_marginal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Kernel</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.classifiers.full_bayesian_pseudo_marginal" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs full Bayesian inference in pseudo-marginal framework</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_train</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>y_train</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>Kernel</strong> (<em>Class</em>) – Class inheriting from gproc.kernel.BaseKernel
Defines family of kernels</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>predict</strong> (<em>function</em>) – Estimates probability of positive given test inputs</p></li>
<li><p><strong>fit_info</strong> (<em>dict</em>) – Dictionary of arbitrary info on the specific fit</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.classifiers.gridsearch_laplace">
<span class="sig-prename descclassname"><span class="pre">gproc.classifiers.</span></span><span class="sig-name descname"><span class="pre">gridsearch_laplace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_fcn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">search_space</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.classifiers.gridsearch_laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Searches over potential hyper parameters and chooses the ones which maxmimise the approximate
marginal log likelihood.
Runs a laplace approximation with the best and provides a function for predicting new x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_train</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>y_train</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>kernel_fcn</strong> (<em>function: N x D numpy array</em><em>, </em><em>N x D numpy array</em><em>, </em><em>dictionary -&gt; N x N numpy array</em>) – </p></li>
<li><p><strong>search_space</strong> (<em>tuple</em>) – Each element is an array corresponding to hyperparameter values to test.
Order must correspond to order in kernel_fcn execution.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>predict</strong> (<em>function</em>) – Estimates probability of positive given test inputs</p></li>
<li><p><strong>fit_info</strong> (<em>dict</em>) – Dictionary of arbitrary info on the specific fit</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="gproc.classifiers.simple_laplace">
<span class="sig-prename descclassname"><span class="pre">gproc.classifiers.</span></span><span class="sig-name descname"><span class="pre">simple_laplace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_fcn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.classifiers.simple_laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs a laplace approximation with a fixed kernel and provides a function for predicting new x.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_train</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>y_train</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>kernel_fcn</strong> (<em>function: N x D numpy array</em><em>, </em><em>N x D numpy array</em><em>, </em><em>dictionary -&gt; N x N numpy array</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>predict</strong> (<em>function</em>) – Estimates probability of positive given test inputs</p></li>
<li><p><strong>fit_info</strong> (<em>dict</em>) – Dictionary of arbitrary info on the specific fit</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-gproc.run_pipeline"></span><dl class="py function">
<dt class="sig sig-object py" id="gproc.run_pipeline.run_pipeline">
<span class="sig-prename descclassname"><span class="pre">gproc.run_pipeline.</span></span><span class="sig-name descname"><span class="pre">run_pipeline</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fit_classifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rhos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([0.,</span> <span class="pre">0.0005005,</span> <span class="pre">0.001001,</span> <span class="pre">0.0015015,</span> <span class="pre">0.002002,</span> <span class="pre">0.0025025,</span> <span class="pre">0.003003,</span> <span class="pre">0.0035035,</span> <span class="pre">0.004004,</span> <span class="pre">0.0045045,</span> <span class="pre">0.00500501,</span> <span class="pre">0.00550551,</span> <span class="pre">0.00600601,</span> <span class="pre">0.00650651,</span> <span class="pre">0.00700701,</span> <span class="pre">0.00750751,</span> <span class="pre">0.00800801,</span> <span class="pre">0.00850851,</span> <span class="pre">0.00900901,</span> <span class="pre">0.00950951,</span> <span class="pre">0.01001001,</span> <span class="pre">0.01051051,</span> <span class="pre">0.01101101,</span> <span class="pre">0.01151151,</span> <span class="pre">0.01201201,</span> <span class="pre">0.01251251,</span> <span class="pre">0.01301301,</span> <span class="pre">0.01351351,</span> <span class="pre">0.01401401,</span> <span class="pre">0.01451451,</span> <span class="pre">0.01501502,</span> <span class="pre">0.01551552,</span> <span class="pre">0.01601602,</span> <span class="pre">0.01651652,</span> <span class="pre">0.01701702,</span> <span class="pre">0.01751752,</span> <span class="pre">0.01801802,</span> <span class="pre">0.01851852,</span> <span class="pre">0.01901902,</span> <span class="pre">0.01951952,</span> <span class="pre">0.02002002,</span> <span class="pre">0.02052052,</span> <span class="pre">0.02102102,</span> <span class="pre">0.02152152,</span> <span class="pre">0.02202202,</span> <span class="pre">0.02252252,</span> <span class="pre">0.02302302,</span> <span class="pre">0.02352352,</span> <span class="pre">0.02402402,</span> <span class="pre">0.02452452,</span> <span class="pre">0.02502503,</span> <span class="pre">0.02552553,</span> <span class="pre">0.02602603,</span> <span class="pre">0.02652653,</span> <span class="pre">0.02702703,</span> <span class="pre">0.02752753,</span> <span class="pre">0.02802803,</span> <span class="pre">0.02852853,</span> <span class="pre">0.02902903,</span> <span class="pre">0.02952953,</span> <span class="pre">0.03003003,</span> <span class="pre">0.03053053,</span> <span class="pre">0.03103103,</span> <span class="pre">0.03153153,</span> <span class="pre">0.03203203,</span> <span class="pre">0.03253253,</span> <span class="pre">0.03303303,</span> <span class="pre">0.03353353,</span> <span class="pre">0.03403403,</span> <span class="pre">0.03453453,</span> <span class="pre">0.03503504,</span> <span class="pre">0.03553554,</span> <span class="pre">0.03603604,</span> <span class="pre">0.03653654,</span> <span class="pre">0.03703704,</span> <span class="pre">0.03753754,</span> <span class="pre">0.03803804,</span> <span class="pre">0.03853854,</span> <span class="pre">0.03903904,</span> <span class="pre">0.03953954,</span> <span class="pre">0.04004004,</span> <span class="pre">0.04054054,</span> <span class="pre">0.04104104,</span> <span class="pre">0.04154154,</span> <span class="pre">0.04204204,</span> <span class="pre">0.04254254,</span> <span class="pre">0.04304304,</span> <span class="pre">0.04354354,</span> <span class="pre">0.04404404,</span> <span class="pre">0.04454454,</span> <span class="pre">0.04504505,</span> <span class="pre">0.04554555,</span> <span class="pre">0.04604605,</span> <span class="pre">0.04654655,</span> <span class="pre">0.04704705,</span> <span class="pre">0.04754755,</span> <span class="pre">0.04804805,</span> <span class="pre">0.04854855,</span> <span class="pre">0.04904905,</span> <span class="pre">0.04954955,</span> <span class="pre">0.05005005,</span> <span class="pre">0.05055055,</span> <span class="pre">0.05105105,</span> <span class="pre">0.05155155,</span> <span class="pre">0.05205205,</span> <span class="pre">0.05255255,</span> <span class="pre">0.05305305,</span> <span class="pre">0.05355355,</span> <span class="pre">0.05405405,</span> <span class="pre">0.05455455,</span> <span class="pre">0.05505506,</span> <span class="pre">0.05555556,</span> <span class="pre">0.05605606,</span> <span class="pre">0.05655656,</span> <span class="pre">0.05705706,</span> <span class="pre">0.05755756,</span> <span class="pre">0.05805806,</span> <span class="pre">0.05855856,</span> <span class="pre">0.05905906,</span> <span class="pre">0.05955956,</span> <span class="pre">0.06006006,</span> <span class="pre">0.06056056,</span> <span class="pre">0.06106106,</span> <span class="pre">0.06156156,</span> <span class="pre">0.06206206,</span> <span class="pre">0.06256256,</span> <span class="pre">0.06306306,</span> <span class="pre">0.06356356,</span> <span class="pre">0.06406406,</span> <span class="pre">0.06456456,</span> <span class="pre">0.06506507,</span> <span class="pre">0.06556557,</span> <span class="pre">0.06606607,</span> <span class="pre">0.06656657,</span> <span class="pre">0.06706707,</span> <span class="pre">0.06756757,</span> <span class="pre">0.06806807,</span> <span class="pre">0.06856857,</span> <span class="pre">0.06906907,</span> <span class="pre">0.06956957,</span> <span class="pre">0.07007007,</span> <span class="pre">0.07057057,</span> <span class="pre">0.07107107,</span> <span class="pre">0.07157157,</span> <span class="pre">0.07207207,</span> <span class="pre">0.07257257,</span> <span class="pre">0.07307307,</span> <span class="pre">0.07357357,</span> <span class="pre">0.07407407,</span> <span class="pre">0.07457457,</span> <span class="pre">0.07507508,</span> <span class="pre">0.07557558,</span> <span class="pre">0.07607608,</span> <span class="pre">0.07657658,</span> <span class="pre">0.07707708,</span> <span class="pre">0.07757758,</span> <span class="pre">0.07807808,</span> <span class="pre">0.07857858,</span> <span class="pre">0.07907908,</span> <span class="pre">0.07957958,</span> <span class="pre">0.08008008,</span> <span class="pre">0.08058058,</span> <span class="pre">0.08108108,</span> <span class="pre">0.08158158,</span> <span class="pre">0.08208208,</span> <span class="pre">0.08258258,</span> <span class="pre">0.08308308,</span> <span class="pre">0.08358358,</span> <span class="pre">0.08408408,</span> <span class="pre">0.08458458,</span> <span class="pre">0.08508509,</span> <span class="pre">0.08558559,</span> <span class="pre">0.08608609,</span> <span class="pre">0.08658659,</span> <span class="pre">0.08708709,</span> <span class="pre">0.08758759,</span> <span class="pre">0.08808809,</span> <span class="pre">0.08858859,</span> <span class="pre">0.08908909,</span> <span class="pre">0.08958959,</span> <span class="pre">0.09009009,</span> <span class="pre">0.09059059,</span> <span class="pre">0.09109109,</span> <span class="pre">0.09159159,</span> <span class="pre">0.09209209,</span> <span class="pre">0.09259259,</span> <span class="pre">0.09309309,</span> <span class="pre">0.09359359,</span> <span class="pre">0.09409409,</span> <span class="pre">0.09459459,</span> <span class="pre">0.0950951,</span> <span class="pre">0.0955956,</span> <span class="pre">0.0960961,</span> <span class="pre">0.0965966,</span> <span class="pre">0.0970971,</span> <span class="pre">0.0975976,</span> <span class="pre">0.0980981,</span> <span class="pre">0.0985986,</span> <span class="pre">0.0990991,</span> <span class="pre">0.0995996,</span> <span class="pre">0.1001001,</span> <span class="pre">0.1006006,</span> <span class="pre">0.1011011,</span> <span class="pre">0.1016016,</span> <span class="pre">0.1021021,</span> <span class="pre">0.1026026,</span> <span class="pre">0.1031031,</span> <span class="pre">0.1036036,</span> <span class="pre">0.1041041,</span> <span class="pre">0.1046046,</span> <span class="pre">0.10510511,</span> <span class="pre">0.10560561,</span> <span class="pre">0.10610611,</span> <span class="pre">0.10660661,</span> <span class="pre">0.10710711,</span> <span class="pre">0.10760761,</span> <span class="pre">0.10810811,</span> <span class="pre">0.10860861,</span> <span class="pre">0.10910911,</span> <span class="pre">0.10960961,</span> <span class="pre">0.11011011,</span> <span class="pre">0.11061061,</span> <span class="pre">0.11111111,</span> <span class="pre">0.11161161,</span> <span class="pre">0.11211211,</span> <span class="pre">0.11261261,</span> <span class="pre">0.11311311,</span> <span class="pre">0.11361361,</span> <span class="pre">0.11411411,</span> <span class="pre">0.11461461,</span> <span class="pre">0.11511512,</span> <span class="pre">0.11561562,</span> <span class="pre">0.11611612,</span> <span class="pre">0.11661662,</span> <span class="pre">0.11711712,</span> <span class="pre">0.11761762,</span> <span class="pre">0.11811812,</span> <span class="pre">0.11861862,</span> <span class="pre">0.11911912,</span> <span class="pre">0.11961962,</span> <span class="pre">0.12012012,</span> <span class="pre">0.12062062,</span> <span class="pre">0.12112112,</span> <span class="pre">0.12162162,</span> <span class="pre">0.12212212,</span> <span class="pre">0.12262262,</span> <span class="pre">0.12312312,</span> <span class="pre">0.12362362,</span> <span class="pre">0.12412412,</span> <span class="pre">0.12462462,</span> <span class="pre">0.12512513,</span> <span class="pre">0.12562563,</span> <span class="pre">0.12612613,</span> <span class="pre">0.12662663,</span> <span class="pre">0.12712713,</span> <span class="pre">0.12762763,</span> <span class="pre">0.12812813,</span> <span class="pre">0.12862863,</span> <span class="pre">0.12912913,</span> <span class="pre">0.12962963,</span> <span class="pre">0.13013013,</span> <span class="pre">0.13063063,</span> <span class="pre">0.13113113,</span> <span class="pre">0.13163163,</span> <span class="pre">0.13213213,</span> <span class="pre">0.13263263,</span> <span class="pre">0.13313313,</span> <span class="pre">0.13363363,</span> <span class="pre">0.13413413,</span> <span class="pre">0.13463463,</span> <span class="pre">0.13513514,</span> <span class="pre">0.13563564,</span> <span class="pre">0.13613614,</span> <span class="pre">0.13663664,</span> <span class="pre">0.13713714,</span> <span class="pre">0.13763764,</span> <span class="pre">0.13813814,</span> <span class="pre">0.13863864,</span> <span class="pre">0.13913914,</span> <span class="pre">0.13963964,</span> <span class="pre">0.14014014,</span> <span class="pre">0.14064064,</span> <span class="pre">0.14114114,</span> <span class="pre">0.14164164,</span> <span class="pre">0.14214214,</span> <span class="pre">0.14264264,</span> <span class="pre">0.14314314,</span> <span class="pre">0.14364364,</span> <span class="pre">0.14414414,</span> <span class="pre">0.14464464,</span> <span class="pre">0.14514515,</span> <span class="pre">0.14564565,</span> <span class="pre">0.14614615,</span> <span class="pre">0.14664665,</span> <span class="pre">0.14714715,</span> <span class="pre">0.14764765,</span> <span class="pre">0.14814815,</span> <span class="pre">0.14864865,</span> <span class="pre">0.14914915,</span> <span class="pre">0.14964965,</span> <span class="pre">0.15015015,</span> <span class="pre">0.15065065,</span> <span class="pre">0.15115115,</span> <span class="pre">0.15165165,</span> <span class="pre">0.15215215,</span> <span class="pre">0.15265265,</span> <span class="pre">0.15315315,</span> <span class="pre">0.15365365,</span> <span class="pre">0.15415415,</span> <span class="pre">0.15465465,</span> <span class="pre">0.15515516,</span> <span class="pre">0.15565566,</span> <span class="pre">0.15615616,</span> <span class="pre">0.15665666,</span> <span class="pre">0.15715716,</span> <span class="pre">0.15765766,</span> <span class="pre">0.15815816,</span> <span class="pre">0.15865866,</span> <span class="pre">0.15915916,</span> <span class="pre">0.15965966,</span> <span class="pre">0.16016016,</span> <span class="pre">0.16066066,</span> <span class="pre">0.16116116,</span> <span class="pre">0.16166166,</span> <span class="pre">0.16216216,</span> <span class="pre">0.16266266,</span> <span class="pre">0.16316316,</span> <span class="pre">0.16366366,</span> <span class="pre">0.16416416,</span> <span class="pre">0.16466466,</span> <span class="pre">0.16516517,</span> <span class="pre">0.16566567,</span> <span class="pre">0.16616617,</span> <span class="pre">0.16666667,</span> <span class="pre">0.16716717,</span> <span class="pre">0.16766767,</span> <span class="pre">0.16816817,</span> <span class="pre">0.16866867,</span> <span class="pre">0.16916917,</span> <span class="pre">0.16966967,</span> <span class="pre">0.17017017,</span> <span class="pre">0.17067067,</span> <span class="pre">0.17117117,</span> <span class="pre">0.17167167,</span> <span class="pre">0.17217217,</span> <span class="pre">0.17267267,</span> <span class="pre">0.17317317,</span> <span class="pre">0.17367367,</span> <span class="pre">0.17417417,</span> <span class="pre">0.17467467,</span> <span class="pre">0.17517518,</span> <span class="pre">0.17567568,</span> <span class="pre">0.17617618,</span> <span class="pre">0.17667668,</span> <span class="pre">0.17717718,</span> <span class="pre">0.17767768,</span> <span class="pre">0.17817818,</span> <span class="pre">0.17867868,</span> <span class="pre">0.17917918,</span> <span class="pre">0.17967968,</span> <span class="pre">0.18018018,</span> <span class="pre">0.18068068,</span> <span class="pre">0.18118118,</span> <span class="pre">0.18168168,</span> <span class="pre">0.18218218,</span> <span class="pre">0.18268268,</span> <span class="pre">0.18318318,</span> <span class="pre">0.18368368,</span> <span class="pre">0.18418418,</span> <span class="pre">0.18468468,</span> <span class="pre">0.18518519,</span> <span class="pre">0.18568569,</span> <span class="pre">0.18618619,</span> <span class="pre">0.18668669,</span> <span class="pre">0.18718719,</span> <span class="pre">0.18768769,</span> <span class="pre">0.18818819,</span> <span class="pre">0.18868869,</span> <span class="pre">0.18918919,</span> <span class="pre">0.18968969,</span> <span class="pre">0.19019019,</span> <span class="pre">0.19069069,</span> <span class="pre">0.19119119,</span> <span class="pre">0.19169169,</span> <span class="pre">0.19219219,</span> <span class="pre">0.19269269,</span> <span class="pre">0.19319319,</span> <span class="pre">0.19369369,</span> <span class="pre">0.19419419,</span> <span class="pre">0.19469469,</span> <span class="pre">0.1951952,</span> <span class="pre">0.1956957,</span> <span class="pre">0.1961962,</span> <span class="pre">0.1966967,</span> <span class="pre">0.1971972,</span> <span class="pre">0.1976977,</span> <span class="pre">0.1981982,</span> <span class="pre">0.1986987,</span> <span class="pre">0.1991992,</span> <span class="pre">0.1996997,</span> <span class="pre">0.2002002,</span> <span class="pre">0.2007007,</span> <span class="pre">0.2012012,</span> <span class="pre">0.2017017,</span> <span class="pre">0.2022022,</span> <span class="pre">0.2027027,</span> <span class="pre">0.2032032,</span> <span class="pre">0.2037037,</span> <span class="pre">0.2042042,</span> <span class="pre">0.2047047,</span> <span class="pre">0.20520521,</span> <span class="pre">0.20570571,</span> <span class="pre">0.20620621,</span> <span class="pre">0.20670671,</span> <span class="pre">0.20720721,</span> <span class="pre">0.20770771,</span> <span class="pre">0.20820821,</span> <span class="pre">0.20870871,</span> <span class="pre">0.20920921,</span> <span class="pre">0.20970971,</span> <span class="pre">0.21021021,</span> <span class="pre">0.21071071,</span> <span class="pre">0.21121121,</span> <span class="pre">0.21171171,</span> <span class="pre">0.21221221,</span> <span class="pre">0.21271271,</span> <span class="pre">0.21321321,</span> <span class="pre">0.21371371,</span> <span class="pre">0.21421421,</span> <span class="pre">0.21471471,</span> <span class="pre">0.21521522,</span> <span class="pre">0.21571572,</span> <span class="pre">0.21621622,</span> <span class="pre">0.21671672,</span> <span class="pre">0.21721722,</span> <span class="pre">0.21771772,</span> <span class="pre">0.21821822,</span> <span class="pre">0.21871872,</span> <span class="pre">0.21921922,</span> <span class="pre">0.21971972,</span> <span class="pre">0.22022022,</span> <span class="pre">0.22072072,</span> <span class="pre">0.22122122,</span> <span class="pre">0.22172172,</span> <span class="pre">0.22222222,</span> <span class="pre">0.22272272,</span> <span class="pre">0.22322322,</span> <span class="pre">0.22372372,</span> <span class="pre">0.22422422,</span> <span class="pre">0.22472472,</span> <span class="pre">0.22522523,</span> <span class="pre">0.22572573,</span> <span class="pre">0.22622623,</span> <span class="pre">0.22672673,</span> <span class="pre">0.22722723,</span> <span class="pre">0.22772773,</span> <span class="pre">0.22822823,</span> <span class="pre">0.22872873,</span> <span class="pre">0.22922923,</span> <span class="pre">0.22972973,</span> <span class="pre">0.23023023,</span> <span class="pre">0.23073073,</span> <span class="pre">0.23123123,</span> <span class="pre">0.23173173,</span> <span class="pre">0.23223223,</span> <span class="pre">0.23273273,</span> <span class="pre">0.23323323,</span> <span class="pre">0.23373373,</span> <span class="pre">0.23423423,</span> <span class="pre">0.23473473,</span> <span class="pre">0.23523524,</span> <span class="pre">0.23573574,</span> <span class="pre">0.23623624,</span> <span class="pre">0.23673674,</span> <span class="pre">0.23723724,</span> <span class="pre">0.23773774,</span> <span class="pre">0.23823824,</span> <span class="pre">0.23873874,</span> <span class="pre">0.23923924,</span> <span class="pre">0.23973974,</span> <span class="pre">0.24024024,</span> <span class="pre">0.24074074,</span> <span class="pre">0.24124124,</span> <span class="pre">0.24174174,</span> <span class="pre">0.24224224,</span> <span class="pre">0.24274274,</span> <span class="pre">0.24324324,</span> <span class="pre">0.24374374,</span> <span class="pre">0.24424424,</span> <span class="pre">0.24474474,</span> <span class="pre">0.24524525,</span> <span class="pre">0.24574575,</span> <span class="pre">0.24624625,</span> <span class="pre">0.24674675,</span> <span class="pre">0.24724725,</span> <span class="pre">0.24774775,</span> <span class="pre">0.24824825,</span> <span class="pre">0.24874875,</span> <span class="pre">0.24924925,</span> <span class="pre">0.24974975,</span> <span class="pre">0.25025025,</span> <span class="pre">0.25075075,</span> <span class="pre">0.25125125,</span> <span class="pre">0.25175175,</span> <span class="pre">0.25225225,</span> <span class="pre">0.25275275,</span> <span class="pre">0.25325325,</span> <span class="pre">0.25375375,</span> <span class="pre">0.25425425,</span> <span class="pre">0.25475475,</span> <span class="pre">0.25525526,</span> <span class="pre">0.25575576,</span> <span class="pre">0.25625626,</span> <span class="pre">0.25675676,</span> <span class="pre">0.25725726,</span> <span class="pre">0.25775776,</span> <span class="pre">0.25825826,</span> <span class="pre">0.25875876,</span> <span class="pre">0.25925926,</span> <span class="pre">0.25975976,</span> <span class="pre">0.26026026,</span> <span class="pre">0.26076076,</span> <span class="pre">0.26126126,</span> <span class="pre">0.26176176,</span> <span class="pre">0.26226226,</span> <span class="pre">0.26276276,</span> <span class="pre">0.26326326,</span> <span class="pre">0.26376376,</span> <span class="pre">0.26426426,</span> <span class="pre">0.26476476,</span> <span class="pre">0.26526527,</span> <span class="pre">0.26576577,</span> <span class="pre">0.26626627,</span> <span class="pre">0.26676677,</span> <span class="pre">0.26726727,</span> <span class="pre">0.26776777,</span> <span class="pre">0.26826827,</span> <span class="pre">0.26876877,</span> <span class="pre">0.26926927,</span> <span class="pre">0.26976977,</span> <span class="pre">0.27027027,</span> <span class="pre">0.27077077,</span> <span class="pre">0.27127127,</span> <span class="pre">0.27177177,</span> <span class="pre">0.27227227,</span> <span class="pre">0.27277277,</span> <span class="pre">0.27327327,</span> <span class="pre">0.27377377,</span> <span class="pre">0.27427427,</span> <span class="pre">0.27477477,</span> <span class="pre">0.27527528,</span> <span class="pre">0.27577578,</span> <span class="pre">0.27627628,</span> <span class="pre">0.27677678,</span> <span class="pre">0.27727728,</span> <span class="pre">0.27777778,</span> <span class="pre">0.27827828,</span> <span class="pre">0.27877878,</span> <span class="pre">0.27927928,</span> <span class="pre">0.27977978,</span> <span class="pre">0.28028028,</span> <span class="pre">0.28078078,</span> <span class="pre">0.28128128,</span> <span class="pre">0.28178178,</span> <span class="pre">0.28228228,</span> <span class="pre">0.28278278,</span> <span class="pre">0.28328328,</span> <span class="pre">0.28378378,</span> <span class="pre">0.28428428,</span> <span class="pre">0.28478478,</span> <span class="pre">0.28528529,</span> <span class="pre">0.28578579,</span> <span class="pre">0.28628629,</span> <span class="pre">0.28678679,</span> <span class="pre">0.28728729,</span> <span class="pre">0.28778779,</span> <span class="pre">0.28828829,</span> <span class="pre">0.28878879,</span> <span class="pre">0.28928929,</span> <span class="pre">0.28978979,</span> <span class="pre">0.29029029,</span> <span class="pre">0.29079079,</span> <span class="pre">0.29129129,</span> <span class="pre">0.29179179,</span> <span class="pre">0.29229229,</span> <span class="pre">0.29279279,</span> <span class="pre">0.29329329,</span> <span class="pre">0.29379379,</span> <span class="pre">0.29429429,</span> <span class="pre">0.29479479,</span> <span class="pre">0.2952953,</span> <span class="pre">0.2957958,</span> <span class="pre">0.2962963,</span> <span class="pre">0.2967968,</span> <span class="pre">0.2972973,</span> <span class="pre">0.2977978,</span> <span class="pre">0.2982983,</span> <span class="pre">0.2987988,</span> <span class="pre">0.2992993,</span> <span class="pre">0.2997998,</span> <span class="pre">0.3003003,</span> <span class="pre">0.3008008,</span> <span class="pre">0.3013013,</span> <span class="pre">0.3018018,</span> <span class="pre">0.3023023,</span> <span class="pre">0.3028028,</span> <span class="pre">0.3033033,</span> <span class="pre">0.3038038,</span> <span class="pre">0.3043043,</span> <span class="pre">0.3048048,</span> <span class="pre">0.30530531,</span> <span class="pre">0.30580581,</span> <span class="pre">0.30630631,</span> <span class="pre">0.30680681,</span> <span class="pre">0.30730731,</span> <span class="pre">0.30780781,</span> <span class="pre">0.30830831,</span> <span class="pre">0.30880881,</span> <span class="pre">0.30930931,</span> <span class="pre">0.30980981,</span> <span class="pre">0.31031031,</span> <span class="pre">0.31081081,</span> <span class="pre">0.31131131,</span> <span class="pre">0.31181181,</span> <span class="pre">0.31231231,</span> <span class="pre">0.31281281,</span> <span class="pre">0.31331331,</span> <span class="pre">0.31381381,</span> <span class="pre">0.31431431,</span> <span class="pre">0.31481481,</span> <span class="pre">0.31531532,</span> <span class="pre">0.31581582,</span> <span class="pre">0.31631632,</span> <span class="pre">0.31681682,</span> <span class="pre">0.31731732,</span> <span class="pre">0.31781782,</span> <span class="pre">0.31831832,</span> <span class="pre">0.31881882,</span> <span class="pre">0.31931932,</span> <span class="pre">0.31981982,</span> <span class="pre">0.32032032,</span> <span class="pre">0.32082082,</span> <span class="pre">0.32132132,</span> <span class="pre">0.32182182,</span> <span class="pre">0.32232232,</span> <span class="pre">0.32282282,</span> <span class="pre">0.32332332,</span> <span class="pre">0.32382382,</span> <span class="pre">0.32432432,</span> <span class="pre">0.32482482,</span> <span class="pre">0.32532533,</span> <span class="pre">0.32582583,</span> <span class="pre">0.32632633,</span> <span class="pre">0.32682683,</span> <span class="pre">0.32732733,</span> <span class="pre">0.32782783,</span> <span class="pre">0.32832833,</span> <span class="pre">0.32882883,</span> <span class="pre">0.32932933,</span> <span class="pre">0.32982983,</span> <span class="pre">0.33033033,</span> <span class="pre">0.33083083,</span> <span class="pre">0.33133133,</span> <span class="pre">0.33183183,</span> <span class="pre">0.33233233,</span> <span class="pre">0.33283283,</span> <span class="pre">0.33333333,</span> <span class="pre">0.33383383,</span> <span class="pre">0.33433433,</span> <span class="pre">0.33483483,</span> <span class="pre">0.33533534,</span> <span class="pre">0.33583584,</span> <span class="pre">0.33633634,</span> <span class="pre">0.33683684,</span> <span class="pre">0.33733734,</span> <span class="pre">0.33783784,</span> <span class="pre">0.33833834,</span> <span class="pre">0.33883884,</span> <span class="pre">0.33933934,</span> <span class="pre">0.33983984,</span> <span class="pre">0.34034034,</span> <span class="pre">0.34084084,</span> <span class="pre">0.34134134,</span> <span class="pre">0.34184184,</span> <span class="pre">0.34234234,</span> <span class="pre">0.34284284,</span> <span class="pre">0.34334334,</span> <span class="pre">0.34384384,</span> <span class="pre">0.34434434,</span> <span class="pre">0.34484484,</span> <span class="pre">0.34534535,</span> <span class="pre">0.34584585,</span> <span class="pre">0.34634635,</span> <span class="pre">0.34684685,</span> <span class="pre">0.34734735,</span> <span class="pre">0.34784785,</span> <span class="pre">0.34834835,</span> <span class="pre">0.34884885,</span> <span class="pre">0.34934935,</span> <span class="pre">0.34984985,</span> <span class="pre">0.35035035,</span> <span class="pre">0.35085085,</span> <span class="pre">0.35135135,</span> <span class="pre">0.35185185,</span> <span class="pre">0.35235235,</span> <span class="pre">0.35285285,</span> <span class="pre">0.35335335,</span> <span class="pre">0.35385385,</span> <span class="pre">0.35435435,</span> <span class="pre">0.35485485,</span> <span class="pre">0.35535536,</span> <span class="pre">0.35585586,</span> <span class="pre">0.35635636,</span> <span class="pre">0.35685686,</span> <span class="pre">0.35735736,</span> <span class="pre">0.35785786,</span> <span class="pre">0.35835836,</span> <span class="pre">0.35885886,</span> <span class="pre">0.35935936,</span> <span class="pre">0.35985986,</span> <span class="pre">0.36036036,</span> <span class="pre">0.36086086,</span> <span class="pre">0.36136136,</span> <span class="pre">0.36186186,</span> <span class="pre">0.36236236,</span> <span class="pre">0.36286286,</span> <span class="pre">0.36336336,</span> <span class="pre">0.36386386,</span> <span class="pre">0.36436436,</span> <span class="pre">0.36486486,</span> <span class="pre">0.36536537,</span> <span class="pre">0.36586587,</span> <span class="pre">0.36636637,</span> <span class="pre">0.36686687,</span> <span class="pre">0.36736737,</span> <span class="pre">0.36786787,</span> <span class="pre">0.36836837,</span> <span class="pre">0.36886887,</span> <span class="pre">0.36936937,</span> <span class="pre">0.36986987,</span> <span class="pre">0.37037037,</span> <span class="pre">0.37087087,</span> <span class="pre">0.37137137,</span> <span class="pre">0.37187187,</span> <span class="pre">0.37237237,</span> <span class="pre">0.37287287,</span> <span class="pre">0.37337337,</span> <span class="pre">0.37387387,</span> <span class="pre">0.37437437,</span> <span class="pre">0.37487487,</span> <span class="pre">0.37537538,</span> <span class="pre">0.37587588,</span> <span class="pre">0.37637638,</span> <span class="pre">0.37687688,</span> <span class="pre">0.37737738,</span> <span class="pre">0.37787788,</span> <span class="pre">0.37837838,</span> <span class="pre">0.37887888,</span> <span class="pre">0.37937938,</span> <span class="pre">0.37987988,</span> <span class="pre">0.38038038,</span> <span class="pre">0.38088088,</span> <span class="pre">0.38138138,</span> <span class="pre">0.38188188,</span> <span class="pre">0.38238238,</span> <span class="pre">0.38288288,</span> <span class="pre">0.38338338,</span> <span class="pre">0.38388388,</span> <span class="pre">0.38438438,</span> <span class="pre">0.38488488,</span> <span class="pre">0.38538539,</span> <span class="pre">0.38588589,</span> <span class="pre">0.38638639,</span> <span class="pre">0.38688689,</span> <span class="pre">0.38738739,</span> <span class="pre">0.38788789,</span> <span class="pre">0.38838839,</span> <span class="pre">0.38888889,</span> <span class="pre">0.38938939,</span> <span class="pre">0.38988989,</span> <span class="pre">0.39039039,</span> <span class="pre">0.39089089,</span> <span class="pre">0.39139139,</span> <span class="pre">0.39189189,</span> <span class="pre">0.39239239,</span> <span class="pre">0.39289289,</span> <span class="pre">0.39339339,</span> <span class="pre">0.39389389,</span> <span class="pre">0.39439439,</span> <span class="pre">0.39489489,</span> <span class="pre">0.3953954,</span> <span class="pre">0.3958959,</span> <span class="pre">0.3963964,</span> <span class="pre">0.3968969,</span> <span class="pre">0.3973974,</span> <span class="pre">0.3978979,</span> <span class="pre">0.3983984,</span> <span class="pre">0.3988989,</span> <span class="pre">0.3993994,</span> <span class="pre">0.3998999,</span> <span class="pre">0.4004004,</span> <span class="pre">0.4009009,</span> <span class="pre">0.4014014,</span> <span class="pre">0.4019019,</span> <span class="pre">0.4024024,</span> <span class="pre">0.4029029,</span> <span class="pre">0.4034034,</span> <span class="pre">0.4039039,</span> <span class="pre">0.4044044,</span> <span class="pre">0.4049049,</span> <span class="pre">0.40540541,</span> <span class="pre">0.40590591,</span> <span class="pre">0.40640641,</span> <span class="pre">0.40690691,</span> <span class="pre">0.40740741,</span> <span class="pre">0.40790791,</span> <span class="pre">0.40840841,</span> <span class="pre">0.40890891,</span> <span class="pre">0.40940941,</span> <span class="pre">0.40990991,</span> <span class="pre">0.41041041,</span> <span class="pre">0.41091091,</span> <span class="pre">0.41141141,</span> <span class="pre">0.41191191,</span> <span class="pre">0.41241241,</span> <span class="pre">0.41291291,</span> <span class="pre">0.41341341,</span> <span class="pre">0.41391391,</span> <span class="pre">0.41441441,</span> <span class="pre">0.41491491,</span> <span class="pre">0.41541542,</span> <span class="pre">0.41591592,</span> <span class="pre">0.41641642,</span> <span class="pre">0.41691692,</span> <span class="pre">0.41741742,</span> <span class="pre">0.41791792,</span> <span class="pre">0.41841842,</span> <span class="pre">0.41891892,</span> <span class="pre">0.41941942,</span> <span class="pre">0.41991992,</span> <span class="pre">0.42042042,</span> <span class="pre">0.42092092,</span> <span class="pre">0.42142142,</span> <span class="pre">0.42192192,</span> <span class="pre">0.42242242,</span> <span class="pre">0.42292292,</span> <span class="pre">0.42342342,</span> <span class="pre">0.42392392,</span> <span class="pre">0.42442442,</span> <span class="pre">0.42492492,</span> <span class="pre">0.42542543,</span> <span class="pre">0.42592593,</span> <span class="pre">0.42642643,</span> <span class="pre">0.42692693,</span> <span class="pre">0.42742743,</span> <span class="pre">0.42792793,</span> <span class="pre">0.42842843,</span> <span class="pre">0.42892893,</span> <span class="pre">0.42942943,</span> <span class="pre">0.42992993,</span> <span class="pre">0.43043043,</span> <span class="pre">0.43093093,</span> <span class="pre">0.43143143,</span> <span class="pre">0.43193193,</span> <span class="pre">0.43243243,</span> <span class="pre">0.43293293,</span> <span class="pre">0.43343343,</span> <span class="pre">0.43393393,</span> <span class="pre">0.43443443,</span> <span class="pre">0.43493493,</span> <span class="pre">0.43543544,</span> <span class="pre">0.43593594,</span> <span class="pre">0.43643644,</span> <span class="pre">0.43693694,</span> <span class="pre">0.43743744,</span> <span class="pre">0.43793794,</span> <span class="pre">0.43843844,</span> <span class="pre">0.43893894,</span> <span class="pre">0.43943944,</span> <span class="pre">0.43993994,</span> <span class="pre">0.44044044,</span> <span class="pre">0.44094094,</span> <span class="pre">0.44144144,</span> <span class="pre">0.44194194,</span> <span class="pre">0.44244244,</span> <span class="pre">0.44294294,</span> <span class="pre">0.44344344,</span> <span class="pre">0.44394394,</span> <span class="pre">0.44444444,</span> <span class="pre">0.44494494,</span> <span class="pre">0.44544545,</span> <span class="pre">0.44594595,</span> <span class="pre">0.44644645,</span> <span class="pre">0.44694695,</span> <span class="pre">0.44744745,</span> <span class="pre">0.44794795,</span> <span class="pre">0.44844845,</span> <span class="pre">0.44894895,</span> <span class="pre">0.44944945,</span> <span class="pre">0.44994995,</span> <span class="pre">0.45045045,</span> <span class="pre">0.45095095,</span> <span class="pre">0.45145145,</span> <span class="pre">0.45195195,</span> <span class="pre">0.45245245,</span> <span class="pre">0.45295295,</span> <span class="pre">0.45345345,</span> <span class="pre">0.45395395,</span> <span class="pre">0.45445445,</span> <span class="pre">0.45495495,</span> <span class="pre">0.45545546,</span> <span class="pre">0.45595596,</span> <span class="pre">0.45645646,</span> <span class="pre">0.45695696,</span> <span class="pre">0.45745746,</span> <span class="pre">0.45795796,</span> <span class="pre">0.45845846,</span> <span class="pre">0.45895896,</span> <span class="pre">0.45945946,</span> <span class="pre">0.45995996,</span> <span class="pre">0.46046046,</span> <span class="pre">0.46096096,</span> <span class="pre">0.46146146,</span> <span class="pre">0.46196196,</span> <span class="pre">0.46246246,</span> <span class="pre">0.46296296,</span> <span class="pre">0.46346346,</span> <span class="pre">0.46396396,</span> <span class="pre">0.46446446,</span> <span class="pre">0.46496496,</span> <span class="pre">0.46546547,</span> <span class="pre">0.46596597,</span> <span class="pre">0.46646647,</span> <span class="pre">0.46696697,</span> <span class="pre">0.46746747,</span> <span class="pre">0.46796797,</span> <span class="pre">0.46846847,</span> <span class="pre">0.46896897,</span> <span class="pre">0.46946947,</span> <span class="pre">0.46996997,</span> <span class="pre">0.47047047,</span> <span class="pre">0.47097097,</span> <span class="pre">0.47147147,</span> <span class="pre">0.47197197,</span> <span class="pre">0.47247247,</span> <span class="pre">0.47297297,</span> <span class="pre">0.47347347,</span> <span class="pre">0.47397397,</span> <span class="pre">0.47447447,</span> <span class="pre">0.47497497,</span> <span class="pre">0.47547548,</span> <span class="pre">0.47597598,</span> <span class="pre">0.47647648,</span> <span class="pre">0.47697698,</span> <span class="pre">0.47747748,</span> <span class="pre">0.47797798,</span> <span class="pre">0.47847848,</span> <span class="pre">0.47897898,</span> <span class="pre">0.47947948,</span> <span class="pre">0.47997998,</span> <span class="pre">0.48048048,</span> <span class="pre">0.48098098,</span> <span class="pre">0.48148148,</span> <span class="pre">0.48198198,</span> <span class="pre">0.48248248,</span> <span class="pre">0.48298298,</span> <span class="pre">0.48348348,</span> <span class="pre">0.48398398,</span> <span class="pre">0.48448448,</span> <span class="pre">0.48498498,</span> <span class="pre">0.48548549,</span> <span class="pre">0.48598599,</span> <span class="pre">0.48648649,</span> <span class="pre">0.48698699,</span> <span class="pre">0.48748749,</span> <span class="pre">0.48798799,</span> <span class="pre">0.48848849,</span> <span class="pre">0.48898899,</span> <span class="pre">0.48948949,</span> <span class="pre">0.48998999,</span> <span class="pre">0.49049049,</span> <span class="pre">0.49099099,</span> <span class="pre">0.49149149,</span> <span class="pre">0.49199199,</span> <span class="pre">0.49249249,</span> <span class="pre">0.49299299,</span> <span class="pre">0.49349349,</span> <span class="pre">0.49399399,</span> <span class="pre">0.49449449,</span> <span class="pre">0.49499499,</span> <span class="pre">0.4954955,</span> <span class="pre">0.495996,</span> <span class="pre">0.4964965,</span> <span class="pre">0.496997,</span> <span class="pre">0.4974975,</span> <span class="pre">0.497998,</span> <span class="pre">0.4984985,</span> <span class="pre">0.498999,</span> <span class="pre">0.4994995,</span> <span class="pre">0.5])</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.run_pipeline.run_pipeline" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits a classifier on train data and compute test statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fit_classifier</strong> (<em>function</em>) – Fits a classifier which will provide a function for predicting unseen
test points</p></li>
<li><p><strong>train_x</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>train_y</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>test_x</strong> (<em>N x D numpy array</em>) – </p></li>
<li><p><strong>test_y</strong> (<em>num_observations x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>cap_metrics</strong> – A variety of abstention based metrics</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy array</p>
</dd>
</dl>
</dd></dl>

<span class="target" id="module-gproc.cross_validation"></span><p>Utility functions for running cross validated classification pipelines</p>
<dl class="py function">
<dt class="sig sig-object py" id="gproc.cross_validation.run_cv">
<span class="sig-prename descclassname"><span class="pre">gproc.cross_validation.</span></span><span class="sig-name descname"><span class="pre">run_cv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classifiers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#gproc.cross_validation.run_cv" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs cross validation for provided x and y data, for each classifier.
Provided splits indicates how many CV folds should be used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_data</strong> (<em>total_n x D numpy array</em>) – </p></li>
<li><p><strong>y_data</strong> (<em>total_n x 1 numpy array</em>) – array containing 1 or -1, the observations</p></li>
<li><p><strong>classifiers</strong> (<em>dict</em>) – key is classifier label
value is “fit_function”, which takes in training x and y and produces
funciton predicting y for new x</p></li>
<li><p><strong>splits</strong> (<em>int</em>) – number of cross validation folds to perform</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>results</strong> – key is classifier label
value is a list containing computed test statistics for each CV fold</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<div class="toctree-wrapper compound">
</div>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">gproc</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Tennessee Hickling, Dominic Broadbent, Edward Milson.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.4.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>